\documentclass[12pt,letterpaper]{exam}
\usepackage[letterpaper,left=0.5in,right=0.5in,top=0.5in,bottom=0.5in]{geometry}
%\usepackage{toc}

% --- Fonts ---
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{libertine}
\usepackage{pifont}

% --- Math ---
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bbm}

% --- References ---
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage[style=ieee]{biblatex}       % bibliographies
\addbibresource{../../Schoolwork/Drafts/Mini Prospectus/other/Prospectus.bib}

% --- Other Common Packages ---
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage{multicol}
\usepackage{graphicx}
\graphicspath{{../figures/}}
\usepackage{subcaption}
\usepackage{placeins}
%\usepackage[shortlabels]{enumitem}
%\usepackage[table]{xcolor}
\usepackage{wrapfig}
%\usepackage{capt-of}
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usetikzlibrary{shapes,arrows,positioning,patterns}
\usepackage{comment}
\usepackage{minted}
\setmintedinline{bgcolor=gray!10,fontsize=\footnotesize}
\setminted[bash]{bgcolor=gray!10,fontsize=\footnotesize}

%\newcommand\chapter{ X }
\renewcommand{\thequestion}{\textbf{\thesection.\arabic{question}}}
\renewcommand{\questionlabel}{\thequestion}

\usepackage{xpatch}	% This hides the multiple label warning from having
\makeatletter		% multiple question sections.
\xpatchcmd{\questions}
  {question@\arabic{question}}
  {question@\arabic{section}@\arabic{question}}
  {}{}
\makeatother

% -------------------------------- Top Matter -------------------------------- %
\newcommand{\class}{PhD Specialty} % This is the name of the course 
\newcommand{\assignmentname}{Examination} % 
\newcommand{\authorname}{Hosley, Brandon} % 
\newcommand{\workdate}{\today} % 
%\printanswers % this includes the solutions sections

% --------------------------------- Document --------------------------------- %
\begin{document}
\pagestyle{plain}
\thispagestyle{empty}
\noindent
 
% ---------------------------------- Header ---------------------------------- %
\noindent
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r @{\extracolsep{10pt}} l}
	\textbf{\class} & \textbf{\authorname} &\\%Your name here instead, obviously
	\textbf{\assignmentname } & \textbf{\workdate} & \\
\end{tabular*}\\ 
\rule{\textwidth}{2pt}

% ----------------------------------- Body ----------------------------------- %
\tableofcontents
\hspace*{1em}
\hrule


\section{Dr. Yielding's Questions}

While HARL is indeed a subfield of MARL, the distinction between the 
two can often be unclear or inconsistently used in the literature. 
In the broadest sense, 'heterogeneous' may refer to any MARL scenario 
where the agents are not identical. However, using this broad 
definition risks diluting the practical significance of the term.

To provide a more useful distinction, HARL should be invoked when 
the heterogeneity of the agents is fundamental, essential, or definitional 
rather than merely incidental. 
This means the differences among agents are critical to their roles and 
interactions within the system, not just minor variations.

Prior to my literature review, I would have considered HARL to apply 
specifically to cases where agents are distinct from the outset, 
either in their capabilities (For action-space \(\mathcal{A}\), 
\(\mathcal{A}_1 \neq \mathcal{A}_2\)) or their observation spaces
(\(\mathcal{O}_1 \neq \mathcal{O}_2\)). 
This intrinsic heterogeneity is evident in works like \cite{calvo2018} 
(the Irish conference paper mentioned in Dr. Yielding's question)
and implicitly reflected in \cite{berner2019} (OpenAI Five), 
even though they do not explicitly label their methods as HARL.

The most comprehensive source on HARL usage is Zhong et al. \cite{zhong2024}, 
whose work has greatly influenced my understanding. 
They focus on implementing algorithms that encourage the development of 
heterogeneous policies among agents. 
Their framework increases the likelihood of individual agents converging 
on distinct policies, which I term emergent heterogeneity.

In my prospectus, I also mention a suspicion that this approach might not 
entirely prevent agents from converging on policies that are functionally 
similar, thus lacking true diversity. However, this assertion remains an 
ancillary detail as it is not yet substantiated by empirical evidence.

Therefore, I propose distinguishing between intrinsic heterogeneity, where 
agents are fundamentally different before training, and emergent heterogeneity, 
where differences arise as a result of the learning process. 
In the cases where the heterogeneity of the agents falls below a level of 
functional relevance, and appears to be incidental, I argue that they
should not be labeled as HARL. By maintaining these distinctions, 
we can more accurately categorize and understand the applications and 
implications of HARL and MARL.

Below, I apply these distinctions to the cases proposed:

\begin{questions}
	\question
	HAA2C and HADDPG are among the numerous algorithms proposed by 
	Zhong et al.~\cite{zhong2024}. While it seems reasonable that their 
	algorithms could be applied to situations with intrinsic heterogeneity, 
	their implementations and tests are applied to environments with agents 
	that are functionally the same.

	I intend to implement (at least a subset of) their algorithms to the 
	extent that time allows. In doing so, I hope to either corroborate or 
	contradict their results by comparing them to similar algorithms under 
	different conditions. This exploration aims to identify any apparent 
	advantages of these algorithms when applied to the experimental variables 
	proposed for Contribution 1. These experimental variables represent 
	smaller difficulties that we expect to face in Contributions 2 and 3.
	% -------------------------- End Question -------------------------- %

	\question 
	\begin{parts}
		\part
		The scenario described in this part of the question is, perhaps 
		unintuitively, more akin to single-agent than multi-agent reinforcement 
		learning. This becomes clearer when considering a single agent acting 
		as an 'overlord,' where the observation space is a combination of 
		observations from all the individual agents. The actions taken by this 
		overlord are combinations of actions chosen for each agent. 
		Essentially, a single policy processes the combined observation and 
		outputs the combined action.

		\part
		This example is a strong example of MARL, and because the agents
		utilize copies of a singular policy, this example is free from
		any of the types of heterogeneity described in the answer for question
		1.1.

		\part
		The types of problems accurately described by the scenario provided 
		in this part of the question appear to be a subset of MARL problems 
		and a superset of HARL problems.

		Many MARL algorithms allow the member policies to develop distinctly 
		(e.g., \cite{foerster2017, rashid2018, lowe2020}), but they are 
		not optimized to facilitate the development of distinct policies.

		Zhong et al. \cite{zhong2024} formulate their series of HARL 
		algorithms with optimizations intended to facilitate the development 
		of distinct policies. One weakness of this formulation is that there 
		is no guarantee that the multiple policies will not converge to a 
		behaviorally indistinct set, similar to the concept of 
		carcinization observed in evolutionary biology.

		Thus, the resulting heterogeneity of the agents in these scenarios 
		is not intrinsic but emergent. Whether the algorithm itself is 
		labeled as MARL or HARL is distinguished by intent.
	\end{parts}
	% -------------------------- End Question -------------------------- %

	\question
	Referencing Centralized Training Decentralized Execution (CTDE) 
	and Decentralized Training Decentralized Execution (DTDE) as employed 
	by Li et al. in their FA2A paper \cite{li2023d}, we see that CTDE is 
	the most common format for Actor-Critic based MARL algorithms 
	\cite{foerster2017, rashid2018, lowe2020, li2023d, zhou2023}.

	Li et al. \cite{li2023d} and Wen et al. \cite{wen2021} are the only 
	papers I found that discuss the contrary implementation, DTDE. 
	In both cases, the authors motivate DTDE with practical concerns, 
	particularly the limitations of inter-agent communication in 
	distributed systems. These considerations are important, but the 
	relation to HARL remains the same as described in answer 1.2 (c).
	% -------------------------- End Question -------------------------- %
\end{questions}


\clearpage
\section{Dr. Robbins' Questions}

\begin{wraptable}{R}{0.45\linewidth}
	\centering
	\vspace*{-1em}
	\begin{tabular}{ll}
		Setting & Value \\
		\midrule
		Template & vscode-server \\
		Image & reg.git.act3-ace.com/ace/ \\
			& hub/vscode-server:v0 \\
		Max CPUs & 64\\
		Req. CPUs & 16 \\
		Memory & 64 GB \\
		GPU & None \\
		\bottomrule
	\end{tabular}
	\caption{Container Settings}
	\label{tab:vm_specs}
	\vspace*{-1em}
\end{wraptable}

In addition to being included in the appendices of this exam,
all code used to run the experiments is available on my github at 
\href{https://github.com/bhosley/Specialty-Exam}{
	https://github.com/bhosley/Specialty-Exam}.
It is written to be run on any ANT-Center VSCode server containers,
but should work in a generic virtual environment.
The specifications for the virtual machines used for this experiment
are enumerated in \cref{tab:vm_specs}.

I recommended increasing the ratio of memory to CPU allocation 
for future experiments as the container was substantially closer
to maximum utilization of memory than processing for the duration
of the experiments run for this exam.

The github readme has a short guide for setting up the virtual
environment in the ANT-Center for easy replication.
The answers for this section are drawn from the experiment running script,
also provided in \cref{app:dqn_exp} of this document.

\begin{questions}
\setcounter{question}{0}
	\question
	The Deep Q-network (DQN) implementation \cite{mnih2013,mnih2015}
	implemented in the RLlib framework \cite{liaw2018tune} was used for 
	each of the answers in this section.
	The default results output is readable by tensorboard, 
	but I chose to use the wandb (Weights and Balances) api as well.
	
	To perform this baseline experiment we can call the default script
	(\cref{app:dqn_exp}). We can accomplish a 5 replication test by setting
	\mintinline{bash}|--num-env-runners=5|, however, this overrides the 
	default of 10, so we have chosen not to use it and to keep the default.
	We use the 
	\mint{bash}|python dqn_exp.py --sweep --num-samples=30 --num-env-runners=10|

	\subsubsection*{Round 1 - Bad Results:}
	In the first round of Parameter sweeping, a large number of training runs
	failed to converge on reasonable results. This was somewhat surprising,
	and when examining the parameters there was no immediately obvious pattern.
	Adding the average number of steps per episode to the Comparison 
	(\cref{fig:baseline_1_para}) shows consistently shorter episodes for poor
	performance.
	\begin{figure}[b]
		\centering
		\includegraphics[width=.95\linewidth]{para_coord_baseline_1.png}
		\caption{Lunar Lander Baseline Experiment 1 Hyperparameters}
		\label{fig:baseline_1_para}
	\end{figure}
	Further investigation showed that the observation space assigned to the 
	environment from the RLlib registered lunar lander environment was
	significantly different from the correct space.
	Further, manually registering the environment after importing
	it directly from the Farama maintained packages using Ray's environment
	registration function resulted in an outdated observation space,
	less prone to resulting in error, but still problematic.

	Thus we gain two actionable insights. First, we adjust this experiment 
	to use the custom Lunar Lander environment described in the next section, 
	but set to one agent. Second, we identify an update that can be performed 
	by a pull request to the maintainers of RLlib, a contribution to the 
	open-source software I would like to attempt after the submission of this
	exam.
	
	\subsubsection*{Round 2 - Baseline Parameter Sweep:}
	To evaluate the parameters of the sweep I elect to  conduct a 
	comparison with three different methods. I will use the same methods 
	for the sweeps performed on the subsequent questions as well.
	
	As above, the parallel coordinates graph provides a visual comparison
	of parameter values and a qualitative impression of the effects.
	Second, I use the wandb api Parameter importance and correlation graph,
	which provides an importance metric based on an algorithm from
	\cite{howard2020deep} that combines a tree-based regression and 
	Spearman rank correlation; and the standard	and Pearson correlation
	metric. Finally, for a more quantitative metric I produce an ANOVA 
	table using second order interactions of the hyperparameters.
	
	\begin{figure}
		\centering
		\includegraphics[width=.95\linewidth]{para_coord_baseline_2.png}
		\caption{Lunar Lander Baseline Experiment 2 Hyperparameters}
		\label{fig:baseline_2_para}
	\end{figure}
	\begin{figure}
		\centering
		\includegraphics[width=.5\linewidth]{importance_baseline_2.png}
		\caption{Baseline Experiment 2 Hyperparameter Importance}
		\label{fig:baseline_2_importance}
	\end{figure}
	
	\Cref{fig:baseline_2_para} shows the parallel coordinates for this 
	experiment's parameter sweep. Immediately this suggests that 
	the n-step instances under-perform the single step instances.
	The factor importance (\cref{fig:baseline_2_importance}) corroborates this.
	Finally, we execute the ANOVA in \cref{tab:anova_baseline},
	and find that the \(\sigma\) parameter has a statistically significant
	effect on the final performance, particular as an interaction variable.
\end{questions}

\subsection*{Multi-Agent Lunar Lander}
\label{sec:ma-lander}
\addcontentsline{toc}{subsection}{\nameref{sec:ma-lander}}

To modify the Lunar Lander environment to support multiple 
agents it appeared to be possible to simply duplicate all of the
sections that referenced the lander in the original code.
However, to achieve cleaner code, and behavior more consistent with 
Farama's PettingZoo environments, I elected to rewrite the environment
to be more consistent with object oriented programming principles.

\begin{figure}[h]
	\begin{subfigure}{.48\textwidth}
		\centering
		\includegraphics[width=.75\linewidth]{single_lander.png}
		\caption{Original}
		\label{fig:original}
	\end{subfigure}
	\begin{subfigure}{.48\textwidth}
		\centering
		\includegraphics[width=.75\linewidth]{dual_lander.png}
		\caption{With 2 Landers}
	\end{subfigure}
	\\
	\begin{subfigure}{.48\textwidth}
		\centering
		\includegraphics[width=.75\linewidth]{penta_lander_1.png}
		\caption{With 5 Landers}
	\end{subfigure}
	\begin{subfigure}{.48\textwidth}
		\centering
		\includegraphics[width=.75\linewidth]{penta_lander_2.png}
		\caption{5 Landers with Collision}
	\end{subfigure}
	\caption{Multi-Agent Lunar Lander}
	\label{fig:landers}
\end{figure}

This trivialized the retention of the environmental physics,
and made it very easy for the new environment to comply with both the 
AEC (Agent Environment Cycle; or sequential) and parallel execution 
methods used in the PettingZoo API. Further, this made it much easier 
to retain the heuristic solution from the original environment.
\Cref{fig:landers} shows several screen shots from the resulting environment
when rendered. 

While an AEC version of this custom environment was constructed,
it was not used for this exam. AEC iterates through the list of agents, 
allowing the agent to select an action and steps the environment with that 
action before moving to the next agent, in a manner similar to a board game.
Such an implement doesn't necessarily contribute value to the 
multi agent Lunar Lander sim, and in fact, causes problems if the physics
of the environment are not adjusted as the number of landers is increased.
Using the heuristic as a measure, the AEC environment becomes unsolvable
with 3 or more landers. The effect of their actions must be scaled to overcome
the wait time between their turns, otherwise they simple crash, unable to 
overcome gravity.

The parallel version of the environment was used for this exam, and required 
only one major assumption, centering around collisions between the landers. 
Lunar Lander uses the Box2D package to model the moon surface and lander.
The most expedient method to address collisions using Box2D was to detect
only contact with the lander's body.
If any other object touches a lander's body it is considered a crash.
As written, the Box2D contact detection does not distinguish between 
what the secondary object is, if the legs of the lander where
included, contact with lunar surface would show as a crash.
Thus I chose not to include the legs contact detection for collisions.
The effective result is that if two lander's legs touch it is not treated
as a crash, however, if one lander's legs touch another's body module
it is treated as a crash and thus a failure.

In addition to being available on the associated github, the new environment
code is included in \cref{app:ma_lander}

\begin{questions}
\setcounter{question}{1}
	\question \textbf{Formulate a Markov Game:}
	The Markov game that represents the multi-agent version of this
	environment is an extension of the Markov Decision Process (MDP)
	that represents the original Lunar Lander environment.
	The formulation presented here will be used to describe the 
	interactions in all of the questions that follow.

	\emph{Agent:} The agent is represented as a member of set of agents 
	\(n\in N\).

	\emph{State Space:} Let \(\mathcal{S}\) be the state space. 
	Then, \(\mathcal{S} \equiv s^N\), where 
	\[s = \begin{cases}
		x \in [-2.5,2.5]   & \text{Position of } n \text{ in } x \\
		y \in [-2.5,2.5]   & \text{Position of } n \text{ in } y \\
		\vec{x} \in [-10,10] & \text{Velocity of } n \text{ in } x \\
		\vec{y} \in [-10,10] & \text{Velocity of } n \text{ in } y \\
		\omega \in [-2\pi,2\pi] & \text{Angle of } n\\
		\vec{\omega} \in [-10,10] & \text{Angular Velocity of } n\\
		\mathbb{I}(\text{leg 1}) \in\{0,1\}  & \text{Leg on ground}\\
		\mathbb{I}(\text{leg 2}) \in\{0,1\}  & \text{Leg on ground}
	\end{cases}\]

	\emph{Action Space:} Let \(\mathcal{A}\) be the action space.
	Then, \(\mathcal{A} \equiv a^N\), where 
	\[a \in \begin{cases}
		0 & \text{No-Op} \\
		1 & \text{Fire Left Engine} \\
		2 & \text{Fire Main Engine} \\
		3 & \text{Fire Right Engine}
	\end{cases}\]

	\emph{Transition Probability:} The transition probability for
	each agent \(n\) remains the same as the original environment,
	\[P_n(s_{n,t+1}|s_{n,t},a_{n,t}) \cong \begin{cases}
		\text{Dispersion} \sim U(-1,1) \\
		\text{Wind(Linear)} = \tanh(\sin(2 k x) + \sin(\pi k x)) \\
		\text{Wind(Rotate)} = \tanh(\sin(2 k x) + \sin(\pi k x)) \\
	\end{cases}\]
	Thus the transition probability for the game as a whole is
	\[P(s_{t+1}|s_{t},a_{t}) = \prod_{n\in N} P_n\]

	\emph{Reward:} The reward structure is similarly retained from 
	the original Lunar Lander environment. Specifically, the reward
	at time \(t\) is defined as
	\(r(t) = \sum_{n\in N} r_n(t)\)
	where
	\begin{align*}
        r_n(t) = \\
        & \pm 100                   & \text{End-state, crash or land} \\
        & +10 (s_{t,6} + s_{t,7})   & \text{leg(s) on ground} \\
        & -a_n(t)\cdot [0,0.03,0.3,0.03] & \text{thruster cost} \\
        & - 100\sqrt{s_{n}(t)_0^2+s_{n}(t)_1^2} & \text{Distance} \\
        & - 100\sqrt{s_{n}(t)_2^2+s_{n}(t)_3^2} & \text{Velocity} \\
        & - 100|\omega_n(t)| & \text{Tilt}
    \end{align*}


    \question \textbf{DQN Single-Agent Controller:}
    This problem was approached using a wrapper class around the
    custom multi-agent environment to translate the between the 
    multi-agent environment and single-agent policy. 
    The proxy single-agent state/observation was formed by simply 
    concatenating the state vectors of each agent.

    The modified action space for this problem is
    \(\mathcal{B} \equiv \mathcal{A}^N\).
    To relate the two action spaces for this problem I use the function,
    \[ b = \sum_{}^{} a_n \times \dim(a)^n. \]
    Then to translate the modified action into the agent-action vector 
    necessary for the multi-agent environment I use the following function,
    \[ \mathbf{a} = \{b/\dim(a)^n \mod\dim(a) \}_{n\in N} \] which 
    relies on \(\dim(\mathcal{A}_n)=\dim(\mathcal{A}_m)\forall\ n,m\in N\),
    an assumption that I note as it is true for this problem, 
    limits some generality.
    The policy can thus be represented as \(\pi(b \big| [s_n]_{n\in N})\).

    Finally, the experiment can be replicated using the command:	
\mint{bash}|python dqn_exp.py --SA --sweep --num-samples=30 --num-env-runners=10|
%
    \begin{figure}
        \centering
        \includegraphics[width=.90\linewidth]{para_coord_sa.png}
        \caption{Single-agent Control Hyperparameter Sweep}
        \label{fig:sa_para}
    \end{figure}
    \begin{figure}
        \centering
        \includegraphics[width=.5\linewidth]{importance_sa.png}
        \caption{Single-agent Control Hyperparameter Importance}
        \label{fig:sa_importance}
    \end{figure}
%
    \Cref{fig:sa_para} shows the results of the parameter sweep for this
    experiment. It appears from this information that there are no clear
    patterns suggesting what the optimal values are for the hyperparameters.
    Next, we consult \cref{fig:sa_importance}. 
    From these metrics we do 
    see some indication that the \(\tau\) and \(n\)-step parameters
    may hold some importance for the final performance.
    And finally, the ANOVA table in \cref{tab:anova_sa}, shows 
    a statistical significance associated with the \(\sigma\)
    parameter. 

    I interpret the high statistical significance but extremely low
    correlation of the \(\sigma\) parameter to suggest that the mid-%
    distribution value has a tendency to produce the best results.
    The superlative instance has a \(\sigma\) parameter near this \(0.5\),
    but overall, it appears that the algorithm is fairly resilient
    to Hyperparameter tuning.


    \question \textbf{NoPS - No Parameter Sharing:}
    This experiment uses a distinct policy for each agent, 
    \(\pi_n(a_n|s_n)\), and can be replicated using:
\mint{bash}|python src/dqn_exp.py --NoPS --sweep --num-samples=30 --num-env-runners=10|
%
    \begin{figure}
        \centering
        \includegraphics[width=.95\linewidth]{para_coord_nops.png}
        \caption{No Parameter Sharing Hyperparameter Sweep}
        \label{fig:nops_para}
    \end{figure}
%
    The parameter sweep results for this experiment can be seen in 
    \cref{fig:nops_para}. There doesn't appear to be any extremely 
    clear or significant patterns from this parallel comparison.

    There are, however, several observations of interest that are not
    directly concerning the hyperparameters. First, is a generally poor 
    performance for instances that produce longer average episode length.
    Second, unlike the parameter sharing control, this consistently converged
    on a result that produced a positive reward in every case.
    I attribute this partially to sample size, but also note that this
    result is consistent with have two separate policies; that in order
    for the final mean episode reward to be negative, both policies would 
    have to converge to poor behavior.
    This may also explain the more even distribution of average returns
    between the instances of this experiment.

    \begin{figure}
        \centering
        \includegraphics[width=.5\linewidth]{importance_nops.png}
        \caption{No Parameter Sharing Hyperparameter Importance}
        \label{fig:nops_importance}
    \end{figure}

    \begin{figure}[H]
        \begin{subfigure}{.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{learn_curve_nops_0.png}
            \caption{Agent 0 Reward}
        \end{subfigure}
        \begin{subfigure}{.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{learn_curve_nops_1.png}
            \caption{Agent 1 Reward}
        \end{subfigure}
        \caption{No-parameter Sharing Learning Curves, 
            average reward \(\times\) training iteration}
        \label{fig:nops_learn_curves}
    \end{figure}

    Next, \cref{fig:nops_importance} shows the parameter importance metrics,
    which similarly suggests no strong relation between the Hyperparameter
    values and results, which is further corroborated in \cref{tab:anova_nops}
    which shows no statistically significant first or second order
    relationship between between the parameters and the final results.
%
    Finally, I present \cref{fig:nops_learn_curves} which shows the learning 
    curves of the agents from 10 instances of this experiment. 
    The resulting curves are consistent with pairs of agents that are
    learning differently. 



    \question \textbf{FuPS - Full Parameter Sharing:}
    This experiment uses a shared policy for each agent, 
    \(\pi(a_n|s_n)\), and can be replicated using:
\mint{bash}|python src/dqn_exp.py --FuPS --sweep --num-samples=30 --num-env-runners=10|

    \begin{figure}[H]
        \centering
        \includegraphics[width=.95\linewidth]{para_coord_fups.png}
        \caption{Full Parameter Sharing Hyperparameter Sweep}
        \label{fig:fups_para}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=.5\linewidth]{importance_fups.png}
        \caption{Full Parameter Sharing Hyperparameter Importance}
        \label{fig:fups_importance}
    \end{figure}

    The parameter sweep results for this experiment can be seen in 
    \cref{fig:fups_para}, once again it appears that there is a 
    slightly higher performance result from a low or no n-step parameter.
    Further, when considering the additional compute required to execute
    the higher \(n\) instances, it is likely to be net negative.
    \Cref{fig:fups_importance} supports the high performance from lower \(n\).
    Further, between the two figures, some support in favor of a \(\tau>0.5\).
    However, the ANOVA results (\cref{tab:anova_fups}) suggest that these
    may not be statistically significant.

\question \textbf{Results Comparisons:}

    \begin{table}[h]
        \centering
        \begin{tabular}{l cccc}
            Control & \(\tau\) & \(\sigma\) & N-Step & Adam-\(\epsilon\) \\
            \midrule
            Single Agent & 0.5 & 0.6 & 1 & 1e-5\\
            NoPS & 0.75 & 0.5 & 1 & 1e-7 \\
            FuPS & 0.75 & 0.5 & 1 & 1e-8 \\
        \end{tabular}
        \caption{Hyperparameters used for control comparisons.}
        \label{tab:comp_parameters}
    \end{table}

For the comparison I ran 10 instances of each algorithm using the 
parameters in \cref{tab:comp_parameters}, which were values near 
those of the superlative performers in the previous sections.
Some consideration was given to the default values of the DQN implementation. 
It may be noted that Mnih et al. \cite{mnih2015} intended for their algorithm 
to be resilient and require minimal adjustments.
The results seen in the previous sections seem to fit that intent,
with exception to certain more extreme settings.

\begin{figure}[b]
    \centering
    \begin{subfigure}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ep_return_min.png}
        \caption{Minimum Return}
    \end{subfigure}
    \hfil
    \begin{subfigure}{.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ep_return_max.png}
        \caption{Maximum Return}
    \end{subfigure}
    \begin{subfigure}{.65\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ep_return_mean.png}
        \caption{Mean Return}
    \end{subfigure}
    \caption{Returns by Episodes}
    \label{fig:ep_returns}
\end{figure}

\Cref{fig:ep_returns} shows learning curve aggregates for each of the 
evaluated algorithms.
From these results we see that the single-agent consistently 
under-performing the other algorithms, particularly, that after 
250 episodes every instance's average was below the average of every 
instance of the other algorithms.
Additionally, the variance in the average performance of the single-agent
control algorithm was significantly greater than the other two algorithms.
I attribute both of these observations to the larger action space and reduced
ability to explore when using this control method.

\begin{figure}
    \centering
    \includegraphics[width=.63\linewidth]{cpu_use.png}
    \caption{CPU Utilization}
    \label{fig:cpu_use}
\end{figure}

\Cref{fig:cpu_use} shows a measure of performance of the algorithms 
from the perspective of computational requirements.
The CPU utilization for the single-agent control was by far the most 
consistent of the three, where the full-parameter sharing was generally
higher while occasionally dropping to a utilization similar to the 
single-agent control. No-parameter sharing appears to have spent equal
time between these two values. 
It is unclear from the experiments conducted for this exam why this would
be the case. I would have expected that the NoPS instance would 
have a generally higher utilization than the others.

\begin{figure}
    \centering
    \includegraphics[width=.63\linewidth]{memory_use.png}
    \caption{Memory Usage (MB)}
    \label{fig:memory_use}
\end{figure}

The memory utilization is about the same for each algorithm 
(\cref{fig:memory_use}). Given that the algorithms use the same deep network 
architecture for their value functions, it reasonably follows that the memory 
usage would be similar, with small differences by the nature of the non-sharing 
networks are two separate networks it would not be surprising for those 
instances to have greater memory use, which appears to be the case by a
small margin.

This same graph highlights perhaps the most significant advantage of 
the single agent control, which was an execution time almost half
as long as the others. While this was not an advantage that I had 
considered prior to the experiment it seems that it would follow
that the single agent controller uses the DQN once per step for each 
action and afterwards updates one network, where as the others would 
have to do so twice.

\end{questions}


\clearpage
\section{Dr. Cox's Questions}
\begin{questions}
    \question

    I concur with the probable necessity of narrowing the scope of 
    contribution one, particularly to maintain the feasibility of the 
    proposed timeline.

    \begin{parts}
        \part \textbf{Cooperative Tasks:}
            The SISL (Stanford Intelligent Systems Laboratory) environments
            are presented by Gupta et. al. in \cite{gupta2017cooperative}.
            Two of the task in particular I believe to be usable, 
            \emph{Pursuit} and \emph{Waterworld}. The latter of the two 
            can be instantiated as a cooperate-competitive task.
            \emph{Multiwalker} may be included, but that will be determined
            by whether or no I am able to get the task to function as
            expected with different numbers of walkers from the default 3.
            The two former environments are known to operate with a wide
            range of numbers of agents.

            The MPE (Multi Particle Environment) is simple and has a variety
            of available tasks. However, among the manufacturer included
            tasks, \emph{Simple Spread} is probably the only one suitable
            for the proposed evaluated. \emph{Simple Spread} is a 
            task that is essentially a zone-coverage/task-assignment
            type problem.

        \part \textbf{Competitive Tasks:}
            To maintain a greater focus on the adaptability and cooperation
            aspect of agent-groups I believe that it would be best to omit
            any pure competitive tasks. 
            However, I intend to include cooperative-competitive tasks.
            As an example of this, let us take \emph{Water World};
            when the task is set such that a single agent can complete
            (`consume the food') it becomes a pure competitive environment,
            however, increasing that value is intended to produce emergent
            cooperation as the goal requires \(n\) agents to simultaneously
            activate the goal.

        \part \textbf{Algorithms:}

        \newcommand{\cmark}{\raisebox{2pt}{\large\hspace{1pt}\ding{51}}}
        \begin{table}[h]
            \centering
            \begin{tabular}{ccccccc}
                \toprule
                Base Algorithm & MARL & HARL & RLlib & RLlib-Contrib & SB3 & Custom \\
                \midrule %\multirow{2}*{PPO}
                PPO & MAPPO &  & \rlap{\(\square\)}{\cmark} \\
                PPO & & HAPPO & & & & \(\square\) \\
                DDPG& MADDPG & & & \(\square\) \\
                DDPG&       & HADDPG & & & & \(\square\) \\
                TRPO& MATRPO & & & & \(\square\) \\
                TRPO&       & HATRPO & & & & \(\square\) \\
                TD3 & MATD3 & & & \(\square\) \\
                TD3 &       & HATD3 & & & & \(\square\) \\
                A2C & MAA2C & & & \(\square\) \\
                A2C &       & HAA2C & & & & \(\square\) \\
                \bottomrule
            \end{tabular}
            \caption{}
            \label{tab:algos}
            \vspace*{-1em}
        \end{table}

            \Cref{tab:algos} outlines the algorithms that I intend to try 
            to utilize for contribution 1. They are broken up as to 
            whether or not the original authors label the algorithm as a 
            MARL of HARL algorithm. Next, they are labeled by a open checkbox 
            corresponding to an open-source implementation that is available 
            for the algorithm. 
            Algorithms label RLlib are included in the base package, 
            the -Contrib column are implementations that have not been fully 
            tested and integrated into the main distribution of the package.
            They work within the \emph{old-api} stack of RLlib, but may require
            some updates to function appropriately with the environments
            that I use.
            %
            The SB3 column refers to Stable-Baselines 3, another 
            well-established open-source library that implements several 
            well known RL algorithms, which may be imported and potentially 
            registered as trainable within the RLlib API. 
            Notably, Trust Region Policy Optimization (TRPO) is the only 
            algorithm that would potentially utilize this style of 
            implementation.

            The custom column represents the algorithms proposed by 
            \cite{zhong2024} which have been implemented in a library of 
            their own, which is publicly available, but standalone. 
            I appreciate that they made their code available, and provided
            instruction on hwo to reproduce their results, however,
            I do believe that there is benefit to conforming to the standards
            of a more popular framework in order to facilitate extension
            and application to other problems.
            The structure of their code is good, and most importantly, 
            consistent, so I anticipate that it should carry over well.
            However, I ran into time constraints, and did not have enough time
            to move the code over and test it appropriately for this exam.
            I intend to do so in the time immediately following.

            The check mark indicates algorithms that have been thoroughly
            tested and function without issue for arbitrary environments
            (provided that the environment is compliant with the API).

        \part \textbf{Environments:}
            \begin{figure}
                \centering
                \includegraphics[width=.63\linewidth]{sisl_envs.png}
                \caption{Multiwalker, pursuit, and waterworld environments}
                \label{fig:sisl_envs}
            \end{figure}
            \begin{table}
                \centering
                \begin{tabular}{ccc}
                    \toprule
                    Environment & Source & RLlib Tested \\
                    \midrule
                    Water World & SISL\cite{gupta2017cooperative} & \rlap{\(\square\)}{\cmark} \\
                    Pursuit & SISL\cite{gupta2017cooperative} & \rlap{\(\square\)}{\cmark} \\
                    Multiwalker & SISL\cite{gupta2017cooperative} & \rlap{\(\square\)}{\cmark} \\
                    Multi-Particle Environment & Petting Zoo & \(\square\) \\
                    SMAX & \cite{rutherford2023} & \(\square\) \\
                    MAgent & \cite{zheng2017} & \(\square\) \\ % https://marllib.readthedocs.io/en/latest/handbook/env.html#air-combat
                    %Close Air Combat &  & \\% https://github.com/liuqh16/CloseAirCombat
                    %GRF  &  & \\
                    %Robot Warehouse &  & \\
                    %Vectorized Soccer &  & \\
                    % &  & \\
                    \bottomrule
                \end{tabular}
                \caption{Candidate Environments}
                \label{tab:enviros}
                \vspace*{-1em}
            \end{table}
            \Cref{tab:enviros} lists the environments that I would like 
            to use as metrics for the algorithms.
            As alluded to in parts (a) and (b), the key feature that I have 
            used in picking these environments is that the variability
            in the number of agents that the task supports.

            The environments that I have confirmed to register without
            issue with the RLlib API are the examples that I have described
            in parts (a) and (b) of this exam question. 
            However, there are other environments that I would like
            to attempt to implement as well. 
            At risk of being overly repetitive, a major advantage of 
            working within an established API provides me the flexibility
            to more easily extend the experiments to additional 
            environments once they are compliant with the API, 
            typically via wrapper.
            Moreover, this provides some flexibility with regard to 
            time and size of contribution 1.

        \part \textbf{League:}
            Evaluation of the league format is definitely something that is
            becoming clearly a scope challenge, particularly with the timeline
            concerns.
            As I have been working on this exam, and exploring the framework
            that I have been using for the experiments, I have been forced to 
            reflect more deeply upon the individual components of the models.
            I have become convinced that league play is something that should
            be evaluated later, with closer proximity to curriculum design than 
            where I am currently.
    \end{parts}

    \question
    The following section is the \emph{mini-paper}. 
    The biggest obstacle currently is, of course, time.
    In the interest of time, the experiment presented does not
    have the replications that I would deem necessary, but should
    serve as a proof of functionality on ACE-hub.

    While working on this exam I have discovered a potential limitation
    with using ACE-hub. Long running scripts appear to generally proceed
    without incident on the containers when disconnected; however, when 
    utilizing an API that opens a port to connect to an external service, 
    disconnecting from the web-based user interface appears to pause execution.
    At the time of submission of this exam I have one workaround, 
    which is to forego exporting the data during the experiment and instead
    to export the results manually afterward.
\end{questions}
\include{mini_paper.tex}

\clearpage

\addcontentsline{toc}{section}{References}
\printbibliography

\clearpage
\appendix

\section{Experiment Running Script}
\label{app:dqn_exp}
\inputminted[
	fontsize=\footnotesize,
	%bgcolor=gray!05,
	linenos
	]{python3}{../src/dqn_exp.py}

\clearpage
\section{Multi-Agent Lunar Lander}
\label{app:ma_lander}
\inputminted[fontsize=\footnotesize, linenos]{python}{../src/multi_lander.py}

\clearpage
\section{ANOVA Tables}
\label{app:anova_tables}
\input{anova_tables.tex}

\end{document}
